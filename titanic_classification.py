# -*- coding: utf-8 -*-
"""Titanic Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PEM9x-Axu5obqmsLrDOPUGdg8X99yDCR

**Overview**

The sinking of the **RMS Titanic** is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we target to complete the analysis of what sorts of people were likely to survive.

<h1>Importing Libraries</h1>
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

sns.set(rc={'figure.figsize':(12, 10)})

"""<h1>Loading Dataset</h1>"""

data = pd.read_csv("train.csv")

data.head(10)

"""**Types of Features :** 
- **Categorical**  - Sex, and Embarked.
- **Continuous **  - Age, Fare
- **Discrete**     - SibSp, Parch.
- **Alphanumeric** - Cabin





"""

data.info()

data.isnull().sum()

data.describe()

"""<h1><font color='blue'>Numerical Value Analysis</font></h1>"""

plt.figure(figsize=(12, 10))
heatmap = sns.heatmap(data[["Survived","SibSp","Parch","Age","Fare"]].corr(), annot=True)

"""**Conclusion**

Fare column has the highest correlation with survival probability but exploration of other features is also needed to determine their relative significance.

<h1><font color="blue">Number of siblings/spouses aboard the Titanic</font></h1>
"""

data['SibSp'].nunique()

data['SibSp'].unique()

bargraph_sib = sns.factorplot(x="SibSp", y="Survived", data=data, kind="bar", size=8)
bargraph_sib = bargraph_sib.set_ylabels("Survival probability")

"""**Conclusion**

Passengers having less siblings(0/1/2) are more likely to survive than passengers with more siblings.

<h1><font color="blue">Age</font></h1>
"""

data['Age'].nunique()

age_vis = sns.FacetGrid(data, col = 'Survived', size=7)
age_vis = age_vis.map(sns.distplot, "Age")
age_vis = age_vis.set_ylabels("Survival probability")

"""**Conclusion**

Age distribution is a gaussian distribution different for survived and not survived subpopulations. Young people have larger chances of survival.

<h1><font color="blue">Sex</font></h1>
"""

plt.figure(figsize=(12,10))
gender_plot = sns.barplot(x="Sex", y="Survived", data=data)
gender_plot = gender_plot.set_ylabel("Survival Probability")

data[["Sex","Survived"]].groupby('Sex').mean()

"""**Conclusion**

Women have larger chances of survival.

<h1><font color="blue">PClass</font></h1>
"""

pclass_plot = sns.factorplot(x="Pclass", y="Survived", data=data, kind="bar", size=8)
pclass_plot = pclass_plot.set_ylabels("Survival Probability")

"""<h1><font color="blue">Pclass vs Survived by Sex</font></h1>


"""

a = sns.factorplot(x="Pclass", y="Survived", hue="Sex", data=data, size=6, kind="bar")
a = a.set_ylabels("survival probability")

"""<h1><font color="blue">Embarked</font></h1>"""

data["Embarked"].isnull().sum()

data["Embarked"].value_counts()

data["Embarked"] = data["Embarked"].fillna("S")

b = sns.factorplot(x="Embarked", y="Survived", data=data, size=7, kind="bar")
b = b.set_ylabels("survival probability")

"""**Conclusion**

Passenger coming from Cherbourg (C) have more chance to survive.

"""

# Explore Pclass vs Embarked 
c = sns.factorplot("Pclass", col="Embarked",  data=data, size=7, kind="count")
c.despine(left=True)
c = c.set_ylabels("Count")

c = sns.factorplot("Sex", col="Embarked",  data=data, size=7, kind="count")

"""**Conclusion**

Cherbourg passengers are mostly in first class which have the highest survival rate.
Southampton (S) and Queenstown (Q) passangers are mostly in third class.

<h1><font color="blue">Preparing data</font></h1>
"""

data.head()

data.info()

mean = data['Age'].mean()

std = data['Age'].std()

is_null = data['Age'].isnull().sum()

# random values between mean, standard deviation and is_null is calculated
rand_age = np.random.randint(mean - std, mean + std, size = is_null)
    
# fill NaN values in Age column with random values generated
age_slice = data["Age"].copy()
age_slice[np.isnan(age_slice)] = rand_age
data["Age"] = age_slice

data.info()

data["Embarked"] = data["Embarked"].fillna("S")

data.drop(['PassengerId','Cabin', 'Ticket','Name'], axis=1, inplace=True)

data.head()

genders = {"male": 0, "female":1}
data['Sex'] = data['Sex'].map(genders)

data.head()

ports = {"S": 0, "C": 1, "Q": 2}
data['Embarked'] = data['Embarked'].map(ports)

data.head()

data.info()

"""<h1><font color="blue">Data Splitting</font></h1>"""

# input and output data

x = data.drop(data.columns[[0]], axis = 1)
y = data['Survived']

x.head()

y.head()

# splitting into training and testing data
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.20, random_state =0)

"""<h1><font color="blue">Feature Scaling</font></h1>"""

from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
xtrain = sc_x.fit_transform(xtrain) 
xtest = sc_x.transform(xtest)

"""<h1><font color="blue">Classification</font></h1>

"""

logreg = LogisticRegression()
svc_classifier = SVC()
dt_classifier = DecisionTreeClassifier()
knn_classifier = KNeighborsClassifier(10)
rf_classifier = RandomForestClassifier(n_estimators=5000, criterion = 'entropy', random_state = 0 )

logreg.fit(xtrain, ytrain)
svc_classifier.fit(xtrain, ytrain)
dt_classifier.fit(xtrain, ytrain)
knn_classifier.fit(xtrain, ytrain)
rf_classifier.fit(xtrain, ytrain)

logreg_ypred = logreg.predict(xtest)
svc_classifier_ypred = svc_classifier.predict(xtest)
dt_classifier_ypred = dt_classifier.predict(xtest)
knn_classifier_ypred = knn_classifier.predict(xtest)
rf_classifier_ypred = rf_classifier.predict(xtest)

from sklearn.metrics import accuracy_score

logreg_acc = accuracy_score(ytest, logreg_ypred)
svc_classifier_acc = accuracy_score(ytest, svc_classifier_ypred)
dt_classifier_acc = accuracy_score(ytest, dt_classifier_ypred)
knn_classifier_acc = accuracy_score(ytest, knn_classifier_ypred)
rf_classifier_acc = accuracy_score(ytest, rf_classifier_ypred)

print ("Logistic Regression : ", round(logreg_acc*100, 2))
print ("Support Vector      : ", round(svc_classifier_acc*100, 2))
print ("Decision Tree       : ", round(dt_classifier_acc*100, 2))
print ("K-NN Classifier     : ", round(knn_classifier_acc*100, 2))
print ("Random Forest       : ", round(rf_classifier_acc*100, 2))

